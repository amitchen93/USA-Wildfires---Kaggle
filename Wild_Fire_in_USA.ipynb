{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport sqlite3\nimport datetime\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"conn = sqlite3.connect(\"/kaggle/input/188-million-us-wildfires/FPA_FOD_20170508.sqlite\")\ndata = pd.read_sql(\"select * from fires\",conn)\nconn.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list(data.columns))\n# COMPLEX_NAME maybe worth checking\nclean_data_v1 = data.drop(['OBJECTID', 'FOD_ID', 'FPA_ID','SOURCE_SYSTEM','SOURCE_SYSTEM_TYPE',\n                          'NWCG_REPORTING_UNIT_ID','NWCG_REPORTING_UNIT_NAME','SOURCE_REPORTING_UNIT','SOURCE_REPORTING_UNIT_NAME',\n                          'LOCAL_FIRE_REPORT_ID','LOCAL_INCIDENT_ID','FIRE_CODE','FIRE_NAME','ICS_209_INCIDENT_NUMBER','ICS_209_NAME',\n                          'MTBS_ID','MTBS_FIRE_NAME','COMPLEX_NAME','FIRE_SIZE_CLASS','Shape','FIPS_CODE','FIPS_NAME'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data_v1.describe()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"clean_data_v1.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## CONT_DATE - DISCOVERY_DATE = FIRE_PERIOD\n\ndf = pd.DataFrame(clean_data_v1['CONT_DATE'])\ndf['DISC_DATE'] = clean_data_v1['DISCOVERY_DATE'] # create df with cont day and discorvery day\ndf['NANS'] = df['CONT_DATE'].isnull()   # create column with true false according to nans value\ndf['NANS'] = df['NANS'].where(df['NANS']==False, df['DISC_DATE']) # replace True values by the discovery day\ndf['CONT_DATE'] = df['CONT_DATE'].fillna(0)  # replace nans values by the 0 in cont day\ndf['CONT_DATE'] = df['CONT_DATE'].where(df['CONT_DATE']!=0, df['NANS']) # replace 0 values by the discovery day in cont day column\nfinal_df_time = df.drop(['NANS'], axis=1)\nfinal_df_time\n\n# print((clean_data_v1['CONT_DATE']).size-((clean_data_v1['CONT_DATE']).dropna()).size)\n# print(len(df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":""},{"metadata":{"trusted":true},"cell_type":"code","source":"# BOX SHIR & LOLA 23/11 + 24/11\n\nimport datetime as dt\nimport math\n\n# CYCLIC COLUMNS FOR DAY OF YEAR\nday_disc = clean_data_v1['DISCOVERY_DOY']\nday_norm = 2 * math.pi * day_disc / day_disc.max()\nclean_data_v1[\"cyclic_day_of_year\"] = np.cos(day_norm)\nclean_data_v1.drop(['DISCOVERY_DOY'], axis=1)\n\n# CYCLIC COLUMNS FOR DAY OF WEEK \ndiscovery_date = pd.to_datetime(clean_data_v1['DISCOVERY_DATE'], unit=\"D\", origin=\"julian\")\nday_of_week = discovery_date.dt.dayofweek\nweek_day_norm = 2 * math.pi * day_of_week\nclean_data_v1[\"cyclic_day_of_week\"] = np.cos(day_of_week / 6)\nclean_data_v1.drop(['DISCOVERY_DATE'], axis=1)\n\n# CYCLIC COLUMNS FOR MONTH \nmonth_of_year = pd.DatetimeIndex(discovery_date).month\nmonth_of_year_norm = 2 * math.pi * (month_of_year-1)\nclean_data_v1[\"cyclic_month_of_year\"] = np.cos(month_of_year_norm / 11)\n\n# ADD FEATURE WEEK END - SEASON\n\n# HOUR Feature\ndetection_hours_array = pd.DataFrame(clean_data_v1['CONT_TIME']).fillna('1400')\ndetection_hours_array['CONT_TIME']= pd.to_numeric(detection_hours_array['CONT_TIME'])\nhours = pd.DataFrame(detection_hours_array['CONT_TIME']/100)\nhours = hours.fillna(14.00)\nhours = hours.replace([np.inf, -np.inf], 14.00)\nhours['CONT_TIME'] = hours['CONT_TIME'].astype(int)\n\n# 'DISCOVERY_TIME'\ndisc_hours = pd.DataFrame(clean_data_v1['DISCOVERY_TIME'])\ndisc_hours['DISCOVERY_TIME']= pd.to_numeric(disc_hours['DISCOVERY_TIME'])\n\n# CONT_DATE - DISCOVERY_DATE = FIRE_PERIOD in term of days\ncont_date = pd.to_datetime(final_df_time['CONT_DATE'], unit=\"D\", origin=\"julian\")\n# clean_data_v1['fire_period'] = cont_date-discovery_date\n# clean_data_v1['fire_period'] = clean_data_v1['fire_period'].dt.days\n\n# CONT_DATE - DISCOVERY_DATE = FIRE_PERIOD in term of hours\ntime_df = pd.DataFrame(discovery_date)\ntime_df['Disc_time'] = clean_data_v1['DISCOVERY_TIME'].fillna('1400')\ntime_df['cont_date'] = cont_date\ndetection_hours_array = pd.DataFrame(clean_data_v1['CONT_TIME']).fillna('1400')\ndetection_hours_array['CONT_TIME'] = detection_hours_array['CONT_TIME'].replace('', '1400')\ntime_df['cont_time'] = detection_hours_array['CONT_TIME']\ntime_df\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FIRE PERIOD in term of hours (2)\n# time_df['Disc_time'] = pd.to_datetime(time_df['Disc_time'], format='%H%M').dt.time\n# time_df['cont_time'] = pd.to_datetime(time_df['cont_time'], format='%H%M').dt.time","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FIRE PERIOD in term of hours (3)\ndate = time_df['DISCOVERY_DATE'].dt.strftime('%Y-%m-%d')\ntime_data_disc = pd.DataFrame(pd.to_datetime(date + ' ' + time_df['Disc_time']))\ndate_2 = time_df['cont_date'].dt.strftime('%Y-%m-%d')\ntime_data_cont = pd.DataFrame(pd.to_datetime(date_2 + ' ' + time_df['cont_time']))\nfire_period = (time_data_cont - time_data_disc).astype('timedelta64[h]')\nfire_period[fire_period < 0] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ODED's CODE\n# Geographical Feature\n# x = np.cos(data[\"LATITUDE\"]) * np.cos(data[\"LONGITUDE\"])\n# y = np.cos(data[\"LATITUDE\"]) * np.sin(data[\"LONGITUDE\"])\n# z = np.sin(data[\"LATITUDE\"])\n\n# # State feature\n# states_dummies = pd.get_dummies(data['STATE'].unique())\n\n# Holidays feature\nfrom pandas.tseries.holiday import USFederalHolidayCalendar as calendar\ndf_holidays = data[['STAT_CAUSE_DESCR', 'DISCOVERY_DATE']].reset_index()\n# discovery_date = pd.to_datetime(clean_data_v1['DISCOVERY_DATE'], unit=\"D\", origin=\"julian\")\n# discovery_date = \n# df_holidays['DISCOVERY_DATE'] = pd.DataFrame(discovery_date)\n\ncal = calendar()\n# print(\"number of holidays:\", len(cal.rules))\n\nholidays_dates = cal.holidays(start=discovery_date.min(), end=discovery_date.max())\ndf_holidays['is_holiday'] = discovery_date.isin(holidays_dates)\ndf_holidays\n\n# holidays_which = cal.holidays(start=discovery_date.min(), end=discovery_date.max(), \n#                               return_name=True).reset_index().rename(columns={\"index\": \"date\", 0: \"holiday\"})\n# holidays_mapping = dict(zip(holidays_which['date'],holidays_which['holiday']))\n# df_holidays['which_holiday'] = df_holidays['DISCOVERY_DATE'].map(holidays_mapping)\n\n# Holidays Graph (1)\n# g = sns.catplot(x='STAT_CAUSE_DESCR', y=\"is_holiday\",data=df_holidays,kind=\"bar\")\n# g.set_xticklabels(rotation=90)\n# g.set(xlabel=\"\", ylabel = \"number of wildfires (%\")\n# g","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Holidays Graph (2)\n# g =  sns.catplot(x=\"which_holiday\", col=\"STAT_CAUSE_DESCR\",data=df_holidays, kind=\"count\", col_wrap=2,height=6, ci=None )\n# g.set_xticklabels(rotation=90)\n# g","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Size Class Feature\nsize_class_dummies = pd.get_dummies(data['FIRE_SIZE_CLASS'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  TAL BOX 3\n# Create 13 features for each reason\n# a = data['STATE']\n# b = data['STAT_CAUSE_DESCR']\n# sum_cause_state_dict = {} # count for each state and each reason \n\n# for i in range(a.shape[0]):\n#     if tuple((a[i],b[i])) not in sum_cause_state_dict:\n#         sum_cause_state_dict[tuple((a[i],b[i]))] = 1\n#     else:\n#         sum_cause_state_dict[tuple((a[i],b[i]))] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TAL BOX 4\n# state_codes = set(data['STATE'])\n# causes = list(set(data['STAT_CAUSE_DESCR']))\n# for code in state_codes:\n#     for cause in causes:\n#         if tuple((code,cause)) not in sum_cause_state_dict:\n#             sum_cause_state_dict[tuple((code,cause))] = 0\n\n# # Create the 13 features \n# for cause in causes:\n#     data['state_' +  cause + '_rate'] = np.zeros(data.shape[0],dtype='float64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TAL BOX 5\n# Relation Matrix between state and reason \n# ratio_mat = np.ndarray((data.shape[0],len(causes)),dtype='float64')\n\n# state_ratio_dict = {}\n# for state in state_codes:\n#     ratios = np.zeros(len(causes), dtype='float64')\n#     for i in range(len(causes)):\n#         ratios[i] = sum_cause_state_dict[tuple((state,causes[i]))]\n#     ratios = ratios / data[data['STATE'] == state].shape[0]\n#     state_ratio_dict[state] = ratios","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TAL BOX 6\n# Filling Relation matrix\n# for i in range(data.shape[0]):\n#     ratio_mat[i] = state_ratio_dict[data['STATE'][i]]\n    \n# for i in range(len(causes)):\n#     data['state_' +  causes[i] + '_rate'] = ratio_mat[:, i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TAL BOX 7 \n# Same processus with MATRIX ratios\n# d = data['OWNER_DESCR']\n# sum_owner_dict = {}\n# for e in d:\n#     if e not in sum_owner_dict:\n#         sum_owner_dict[e] = 1\n#     else:\n#         sum_owner_dict[e] += 1\n# fires_per_owner = np.zeros(d.shape[0])\n# data['fires_owner'] = [sum_owner_dict[val] for val in d]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TAL BOX 8 \n# a = data['OWNER_DESCR']\n# b = data['STAT_CAUSE_DESCR']\n# sum_cause_owner_dict = {}\n# for i in range(a.shape[0]):\n#     if tuple((a[i],b[i])) not in sum_cause_owner_dict:\n#         sum_cause_owner_dict[tuple((a[i],b[i]))] = 1\n#     else:\n#         sum_cause_owner_dict[tuple((a[i],b[i]))] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TAL BOX 9\n# owner_codes = set(data['OWNER_DESCR'])\n# causes = list(set(data['STAT_CAUSE_DESCR']))\n# for code in owner_codes:\n#     for cause in causes:\n#         if tuple((code,cause)) not in sum_cause_owner_dict:\n#             sum_cause_owner_dict[tuple((code,cause))] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TAL BOX 10\n# ratio_mat = np.ndarray((data.shape[0],len(causes)),dtype='float64')\n# owner_ratio_dict = {}\n# for owner in owner_codes:\n#     ratios = np.zeros(len(causes), dtype='float64')\n#     for i in range(len(causes)):\n#         ratios[i] = sum_cause_owner_dict[tuple((owner,causes[i]))]\n#     ratios = ratios / data[data['OWNER_DESCR'] == owner].shape[0]\n#     owner_ratio_dict[owner] = ratios\n\n# for i in range(data.shape[0]):\n#     ratio_mat[i] = owner_ratio_dict[data['OWNER_DESCR'][i]]\n# for i in range(len(causes)):\n#     data['owner_' +  causes[i] + '_rate'] = ratio_mat[:, i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TAL BOX 11\n# Same processus for both features : Owner Description & States VS Reasons\n# Overfitted Feature\ngrouped = data[['OWNER_DESCR','STATE','STAT_CAUSE_DESCR']].groupby(by=['OWNER_DESCR','STATE','STAT_CAUSE_DESCR'])\n\nowner_state_cause_dict = {}\nowners = set(data['OWNER_DESCR'])\nstates = set(data['STATE'])\ncauses = set(data['STAT_CAUSE_DESCR'])\n             \nfor owner in owners:\n    for state in states:\n        for cause in causes:\n            owner_state_cause_dict[tuple((owner,state,cause))] = 0\nfor x,y in grouped:\n    owner_state_cause_dict[tuple((x[0],x[1],x[2]))] = y.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tal box 12\ngrouped = data[['OWNER_DESCR','STATE']].groupby(by=['OWNER_DESCR','STATE'])\nowner_state_dict = {}\nowners = set(data['OWNER_DESCR'])\nstates = set(data['STATE'])\n             \n# For not divide by 0\nfor owner in owners:\n    for state in states:\n        owner_state_dict[tuple((owner, state))] = 1\nlen(owner_state_dict)\n\nfor x,y in grouped:\n    owner_state_dict[tuple((x[0], x[1]))] = y.shape[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LAST BOX TAL\nratio_mat = np.ndarray((data.shape[0],len(causes)),dtype='float64')\nowner_state_ratio_dict = {}\ncauses = list(causes)\nfor owner in owners:\n    for state in states:\n        ratios = np.zeros(len(causes), dtype='float64')\n        sumi = 0\n        for i in range(len(causes)):\n            ratios[i] = owner_state_cause_dict[tuple((owner,state,causes[i]))]\n            sumi += owner_state_cause_dict[tuple((owner,state,causes[i]))]\n        ratios = ratios / owner_state_dict[tuple((owner,state))]\n        owner_state_ratio_dict[tuple((owner,state))] = ratios\n        \nfor i in range(data.shape[0]):\n    owner = data['OWNER_DESCR'][i]\n    state = data['STATE'][i]\n    ratio_mat[i] = owner_state_ratio_dict[tuple((owner,state))]\n    \na = pd.DataFrame(data[['OWNER_DESCR','STATE','STAT_CAUSE_DESCR']])\nfor i in range(len(causes)):\n    data['owner_state_' +  causes[i] + '_rate'] = ratio_mat[:, i]\n    a['owner_state_' +  causes[i] + '_rate'] = ratio_mat[:, i]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Geographic feature\nimport geopandas as gpd\nimport shapely\nfrom shapely.geometry import Point # Point class\nfrom shapely.geometry import shape # shape() is a function to convert geo objects through the interface\nimport gc\n\na = data[['LATITUDE','LONGITUDE']]\nfires = gpd.GeoDataFrame(a.drop(['LATITUDE', 'LONGITUDE'], axis=1),\n                       crs={'init': 'epsg:4326'},\n                       geometry=[shapely.geometry.Point(xy)\n                                 for xy in zip(a.LONGITUDE, a.LATITUDE)])\ncities = gpd.read_file('/kaggle/input/cities/tl_2017_us_uac10.shp')\n\nfires_in_cities = gpd.sjoin(fires, cities, how=\"left\", op='within')\na = fires_in_cities['FUNCSTAT10'].replace('S',1)\na = a.replace(np.nan,0)\ndel cities\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TAL BOX 2\nd = data['STATE'] \nsum_state_dict = {} # number of fires in each state\nfor e in d:\n    if e not in sum_state_dict:\n        sum_state_dict[e] = 1\n    else:\n        sum_state_dict[e] += 1\nfires_per_state = np.zeros(d.shape[0])\ndata['fires_in_state'] = [sum_state_dict[val] for val in d] # new feature","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# FINAL DATA \nlabel = clean_data_v1['STAT_CAUSE_DESCR']\n\nfinal_df = clean_data_v1[\"cyclic_month_of_year\"]\nfinal_df = pd.concat([final_df, size_class_dummies], axis=1)\nfinal_df = pd.concat([final_df, data.iloc[:, 39:52]], axis=1)\nfinal_df['cyclic_day_of_week'] = clean_data_v1['cyclic_day_of_week']\nfinal_df['cyclic_day_of_year'] = clean_data_v1['cyclic_day_of_year'] \nfinal_df['hour_in_day'] = hours['CONT_TIME']\nfinal_df['fire_period'] = fire_period\nfinal_df['is_urban'] = a\nfinal_df['is_holidays'] = df_holidays['is_holiday'].astype(int)\nfinal_df['num_fires_in_state'] = data['fires_in_state']\nfinal_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**MODELS**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n#Import scikit-learn metrics module for accuracy calculation \nfrom sklearn import metrics \nfrom sklearn.metrics import zero_one_loss \nfrom sklearn.model_selection import GridSearchCV \n\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.linear_model import LogisticRegressionCV \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import average_precision_score\n\nnp.random.seed(137)\n\nlabel=data['STAT_CAUSE_DESCR']\nX_train, X_test, y_train, y_test = train_test_split(final_df, label, test_size=0.25) # 75% training and 25% test\n\n#Create a Gaussian Classifier \nclf = xgb.XGBClassifier(max_depth=22,n_estimators=150)\n# clf = RandomForestClassifier(max_depth=20, n_estimators = 150)\nclf.fit(X_train,y_train)\n\n# X_train.info()\n#Train the model using the training sets \ny_pred=clf.predict(X_test)\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n# # average_precision = dict()\n# n_classes = 7\n# for i in range(n_classes):\n#     average_precision[i] = average_precision_score(y_test[:, i], y_pred[:, i])\n\n# A \"micro-average\": quantifying score on all classes jointly\n# average_precision[\"micro\"] = average_precision_score(y_test, y_pred,\n#                                                      average=\"micro\")\n# print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n#       .format(average_precision[\"micro\"]))\n# y_pred\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_score\nprecision_score(y_test, y_pred, average='macro')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import catboost as cb\n# import numpy as np\n# from sklearn.datasets import load_breast_cancer\n# from sklearn.metrics import accuracy_score\n# from sklearn.model_selection import train_test_split\n\n# import optuna\n\n\n# def objective(trial):\n#     label=data['STAT_CAUSE_CODE']\n#     train_x, valid_x, train_y, valid_y = train_test_split(final_df, label, test_size=0.3)\n\n#     param = {\n#         \"objective\": trial.suggest_categorical(\"objective\", [\"MultiClass\"]),\n#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n#         \"depth\": trial.suggest_int(\"depth\", 1, 12),\n#         \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n#         \"bootstrap_type\": trial.suggest_categorical(\n#             \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n#         )\n#     }\n\n#     if param[\"bootstrap_type\"] == \"Bayesian\":\n#         param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n#     elif param[\"bootstrap_type\"] == \"Bernoulli\":\n#         param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n#     gbm = cb.CatBoostClassifier(**param)\n#     print('before fit')\n#     gbm.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=0, early_stopping_rounds=100)\n#     print('after fit')\n\n#     preds = gbm.predict(valid_x)\n#     pred_labels = np.rint(preds)\n#     accuracy = accuracy_score(valid_y, pred_labels)\n#     print(accuracy)\n#     print(\"Precision:\", precision_score(valid_y, pred_labels, average='macro'))\n#     return accuracy\n\n\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=10, show_progress_bar=True)\n\n# print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n# print(\"Best trial:\")\n# trial = study.best_trial\n\n# print(\"  Value: {}\".format(trial.value))\n\n# print(\"  Params: \")\n# for key, value in trial.params.items():\n#     print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Gradient Boosting Model\n# from sklearn.model_selection import train_test_split\n# from sklearn import metrics \n# from sklearn.ensemble import GradientBoostingClassifier \n# from sklearn.model_selection import cross_val_score\n# import optuna \n# from optuna import trial\n# from sklearn.metrics import precision_score\n# from sklearn.pipeline import Pipeline\n\n# label=data['STAT_CAUSE_DESCR']\n# X_train, X_test, y_train, y_test = train_test_split(final_df, label, test_size=0.25)\n# # model = Pipeline([('gbm',GradientBoostingClassifier())])\n\n# def objective(trial):\n\n#     n_estimators =  trial.suggest_int('n_estimators', 2, 20)\n# #     max_features = trial.suggest_int('max_features', 1,20)\n# #     subsample = trial.suggest_uniform('subsample', 0.1, 0.5)\n    \n#     params = {\n#             'n_estimators': n_estimators\n# #             'max_features': max_features,\n# #             'subsample': subsample\n#             }\n#     model = GradientBoostingClassifier()\n#     model.set_params(**params)\n#     return - np.mean(cross_val_score(model, X_train, y_train, cv=8))\n\n# def Optimized_GradientBoostingClassifier():\n#     # optimize hyperparameters\n#     study = optuna.create_study(direction='maximize')\n#     study = study.optimize(objective, n_trials=2, timeout=600, show_progress_bar=True)\n\n#     model.set_params(**study.best_params)\n#     model.fit(X_train, y_train)\n#     y_pred=model.predict(X_test)\n#     print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n#     print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n\n# # def XGB():\n# #     clf = xgboost.XGBClassifier().fit(X_train, y_train)\n# Optimized_GradientBoostingClassifier()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import catboost as cb\n# import numpy as np\n# from sklearn.datasets import load_breast_cancer\n# from sklearn.metrics import accuracy_score\n# from sklearn.model_selection import train_test_split\n\n# import optuna\n\n\n# def objective(trial):\n#     label=data['STAT_CAUSE_CODE']\n#     train_x, valid_x, train_y, valid_y = train_test_split(final_df, label, test_size=0.3)\n\n#     param = {\n#         \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n#         \"depth\": trial.suggest_int(\"depth\", 1, 12),\n#         \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n#         \"bootstrap_type\": trial.suggest_categorical(\n#             \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n#         ),\n#         \"used_ram_limit\": \"3gb\",\n#         \"loss_function\" : 'MultiClass'\n#     }\n\n#     if param[\"bootstrap_type\"] == \"Bayesian\":\n#         param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n#     elif param[\"bootstrap_type\"] == \"Bernoulli\":\n#         param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n#     gbm = cb.CatBoostClassifier(**param)\n\n#     gbm.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=0, early_stopping_rounds=100)\n\n#     preds = gbm.predict(valid_x)\n#     pred_labels = np.rint(preds)\n#     accuracy = accuracy_score(valid_y, pred_labels)\n#     return accuracy\n\n\n# if __name__ == \"__main__\":\n#     study = optuna.create_study(direction=\"maximize\")\n#     study.optimize(objective, n_trials=10, timeout=600, show_progress_bar=True)\n\n#     print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n#     print(\"Best trial:\")\n#     trial = study.best_trial\n\n#     print(\"  Value: {}\".format(trial.value))\n\n#     print(\"  Params: \")\n#     for key, value in trial.params.items():\n#         print(\"    {}: {}\".format(key, value))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import seaborn as sns\n# import matplotlib.pyplot as plt\n# from datetime import datetime\n# import statistics\n# clean_data_heat = clean_data_v1[['FIRE_YEAR', 'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME',\n#        'STAT_CAUSE_CODE', 'CONT_DATE', 'CONT_DOY',\n#        'CONT_TIME', 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE']]\n# heat_names =['FIRE_YEAR', 'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME',\n#        'STAT_CAUSE_CODE', 'CONT_DATE', 'CONT_DOY',\n#        'CONT_TIME', 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE']\n\n# ###\n# detection_hours_array = np.array(np.array((clean_data_heat['CONT_TIME']).dropna(),dtype ='int32')/100,dtype ='int32')\n# detection_hours_mode = statistics.mode(detection_hours_array)\n# ###\n# print(set(((clean_data_heat['CONT_TIME']).dropna())))\n# ###\n# contained_hours_array = np.array(np.array((clean_data_heat['DISCOVERY_TIME']).dropna(),dtype ='int32')/100,dtype ='int32')\n# contained_hours_mode = statistics.mode(contained_hours_array)\n# ###\n# cur = pd.DataFrame(detection_hours_array,columns=['hour'])\n# a = sns.countplot('hour',data=cur)\n# a.set_xticklabels(set(cur['hour']),rotation=45)\n# # \n\n# # for cont_date \n# #     df_test['Difference'] = df_test['First_Date'].sub(df_test['Second Date'], axis=0)\n\n# #     df_test['CONT_DAY'] = df_test['DISCOVERY_DATE'].sub(df_test['CONT_DATE'], axis=0)\n\n\n\n# clean_data_v1 = clean_data_v1.dropna()\n# f,ax=plt.subplots(1,1,figsize=(20,10))\n# sns.heatmap(np.abs(clean_data_v1.corr()),annot=True,cmap='RdYlGn',ax=ax)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":""}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}