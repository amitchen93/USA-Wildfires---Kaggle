{
 "cells": [
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import sqlite3\n",
    "import datetime\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "conn = sqlite3.connect(\"/kaggle/input/188-million-us-wildfires/FPA_FOD_20170508.sqlite\")\n",
    "data = pd.read_sql(\"select * from fires\",conn)\n",
    "conn.close()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "print(list(data.columns))\n",
    "# drop some features after I saw they are not so helpful for prediction\n",
    "clean_data_v1 = data.drop(['OBJECTID', 'FOD_ID', 'FPA_ID','SOURCE_SYSTEM','SOURCE_SYSTEM_TYPE',\n",
    "                          'NWCG_REPORTING_UNIT_ID','NWCG_REPORTING_UNIT_NAME','SOURCE_REPORTING_UNIT','SOURCE_REPORTING_UNIT_NAME',\n",
    "                          'LOCAL_FIRE_REPORT_ID','LOCAL_INCIDENT_ID','FIRE_CODE','FIRE_NAME','ICS_209_INCIDENT_NUMBER','ICS_209_NAME',\n",
    "                          'MTBS_ID','MTBS_FIRE_NAME','COMPLEX_NAME','FIRE_SIZE_CLASS','Shape','FIPS_CODE','FIPS_NAME'],axis=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# take a look at the data\n",
    "clean_data_v1.describe()\n",
    "clean_data_v1.info()\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# CONT_DATE - DISCOVERY_DATE = FIRE_PERIOD\n",
    "\n",
    "df = pd.DataFrame(clean_data_v1['CONT_DATE']) # Date on which the fire was declared contained or otherwise controlled (mm/dd/yyyy)\n",
    "\n",
    "# DISCOVERY_DATE = Date on which the fire was discovered or confirmed to exist\n",
    "df['DISC_DATE'] = clean_data_v1['DISCOVERY_DATE'] # create df with cont day and discovery day\n",
    "df['NANS'] = df['CONT_DATE'].isnull()   # create column with true false according to nans value\n",
    "df['NANS'] = df['NANS'].where(df['NANS']==False, df['DISC_DATE']) # replace True values by the discovery day\n",
    "df['CONT_DATE'] = df['CONT_DATE'].fillna(0)  # replace nans values by the 0 in cont day\n",
    "df['CONT_DATE'] = df['CONT_DATE'].where(df['CONT_DATE']!=0, df['NANS']) # replace 0 values by the discovery day in cont day column\n",
    "final_df_time = df.drop(['NANS'], axis=1)\n",
    "final_df_time\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import datetime as dt\n",
    "import math\n",
    "\n",
    "# reminder: DISCOVERY_DOY = Day of year on which the fire was discovered or confirmed to exist.\n",
    "\n",
    "\n",
    "# CYCLIC COLUMNS FOR DAY OF YEAR\n",
    "day_disc = clean_data_v1['DISCOVERY_DOY']\n",
    "day_norm = 2 * math.pi * day_disc / day_disc.max()\n",
    "clean_data_v1[\"cyclic_day_of_year\"] = np.cos(day_norm)\n",
    "clean_data_v1.drop(['DISCOVERY_DOY'], axis=1)  # axis=1 -> drop columns\n",
    "\n",
    "# CYCLIC COLUMNS FOR DAY OF WEEK\n",
    "\n",
    "# pd.to_datetime: Convert argument to datetime (from table to something like this: 2015-02-04)\n",
    "discovery_date = pd.to_datetime(clean_data_v1['DISCOVERY_DATE'], unit=\"D\", origin=\"julian\")\n",
    "day_of_week = discovery_date.dt.dayofweek\n",
    "week_day_norm = 2 * math.pi * day_of_week\n",
    "clean_data_v1[\"cyclic_day_of_week\"] = np.cos(day_of_week / 6)\n",
    "clean_data_v1.drop(['DISCOVERY_DATE'], axis=1)\n",
    "\n",
    "# CYCLIC COLUMNS FOR MONTH \n",
    "month_of_year = pd.DatetimeIndex(discovery_date).month\n",
    "month_of_year_norm = 2 * math.pi * (month_of_year-1)\n",
    "clean_data_v1[\"cyclic_month_of_year\"] = np.cos(month_of_year_norm / 11)\n",
    "\n",
    "\n",
    "# HOUR Feature\n",
    "detection_hours_array = pd.DataFrame(clean_data_v1['CONT_TIME']).fillna('1400')\n",
    "detection_hours_array['CONT_TIME']= pd.to_numeric(detection_hours_array['CONT_TIME'])\n",
    "hours = pd.DataFrame(detection_hours_array['CONT_TIME']/100)\n",
    "hours = hours.fillna(14.00)\n",
    "hours = hours.replace([np.inf, -np.inf], 14.00)\n",
    "hours['CONT_TIME'] = hours['CONT_TIME'].astype(int)\n",
    "\n",
    "# 'DISCOVERY_TIME'\n",
    "disc_hours = pd.DataFrame(clean_data_v1['DISCOVERY_TIME'])\n",
    "disc_hours['DISCOVERY_TIME']= pd.to_numeric(disc_hours['DISCOVERY_TIME'])\n",
    "\n",
    "# CONT_DATE - DISCOVERY_DATE = FIRE_PERIOD in term of days\n",
    "cont_date = pd.to_datetime(final_df_time['CONT_DATE'], unit=\"D\", origin=\"julian\")\n",
    "# clean_data_v1['fire_period'] = cont_date-discovery_date\n",
    "# clean_data_v1['fire_period'] = clean_data_v1['fire_period'].dt.days\n",
    "\n",
    "# CONT_DATE - DISCOVERY_DATE = FIRE_PERIOD in term of hours\n",
    "time_df = pd.DataFrame(discovery_date)\n",
    "time_df['Disc_time'] = clean_data_v1['DISCOVERY_TIME'].fillna('1400')\n",
    "time_df['cont_date'] = cont_date\n",
    "detection_hours_array = pd.DataFrame(clean_data_v1['CONT_TIME']).fillna('1400')\n",
    "detection_hours_array['CONT_TIME'] = detection_hours_array['CONT_TIME'].replace('', '1400')\n",
    "time_df['cont_time'] = detection_hours_array['CONT_TIME']\n",
    "time_df\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# FIRE PERIOD in term of hours (3)\n",
    "date = time_df['DISCOVERY_DATE'].dt.strftime('%Y-%m-%d')\n",
    "time_data_disc = pd.DataFrame(pd.to_datetime(date + ' ' + time_df['Disc_time']))\n",
    "date_2 = time_df['cont_date'].dt.strftime('%Y-%m-%d')\n",
    "time_data_cont = pd.DataFrame(pd.to_datetime(date_2 + ' ' + time_df['cont_time']))\n",
    "fire_period = (time_data_cont - time_data_disc).astype('timedelta64[h]')\n",
    "fire_period[fire_period < 0] = 0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Geographical Feature\n",
    "# x = np.cos(data[\"LATITUDE\"]) * np.cos(data[\"LONGITUDE\"])\n",
    "# y = np.cos(data[\"LATITUDE\"]) * np.sin(data[\"LONGITUDE\"])\n",
    "# z = np.sin(data[\"LATITUDE\"])\n",
    "\n",
    "# # State feature\n",
    "# states_dummies = pd.get_dummies(data['STATE'].unique())\n",
    "\n",
    "# Holidays feature\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "df_holidays = data[['STAT_CAUSE_DESCR', 'DISCOVERY_DATE']].reset_index()\n",
    "# discovery_date = pd.to_datetime(clean_data_v1['DISCOVERY_DATE'], unit=\"D\", origin=\"julian\")\n",
    "# discovery_date = \n",
    "# df_holidays['DISCOVERY_DATE'] = pd.DataFrame(discovery_date)\n",
    "\n",
    "cal = calendar()\n",
    "# print(\"number of holidays:\", len(cal.rules))\n",
    "\n",
    "holidays_dates = cal.holidays(start=discovery_date.min(), end=discovery_date.max())\n",
    "df_holidays['is_holiday'] = discovery_date.isin(holidays_dates)\n",
    "df_holidays\n",
    "\n",
    "# holidays_which = cal.holidays(start=discovery_date.min(), end=discovery_date.max(), \n",
    "#                               return_name=True).reset_index().rename(columns={\"index\": \"date\", 0: \"holiday\"})\n",
    "# holidays_mapping = dict(zip(holidays_which['date'],holidays_which['holiday']))\n",
    "# df_holidays['which_holiday'] = df_holidays['DISCOVERY_DATE'].map(holidays_mapping)\n",
    "\n",
    "# Holidays Graph (1)\n",
    "# g = sns.catplot(x='STAT_CAUSE_DESCR', y=\"is_holiday\",data=df_holidays,kind=\"bar\")\n",
    "# g.set_xticklabels(rotation=90)\n",
    "# g.set(xlabel=\"\", ylabel = \"number of wildfires (%\")\n",
    "# g"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Size Class Feature\n",
    "size_class_dummies = pd.get_dummies(data['FIRE_SIZE_CLASS'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Create 13 features for each reason\n",
    "# a = data['STATE']\n",
    "# b = data['STAT_CAUSE_DESCR']\n",
    "# sum_cause_state_dict = {} # count for each state and each reason \n",
    "\n",
    "# for i in range(a.shape[0]):\n",
    "#     if tuple((a[i],b[i])) not in sum_cause_state_dict:\n",
    "#         sum_cause_state_dict[tuple((a[i],b[i]))] = 1\n",
    "#     else:\n",
    "#         sum_cause_state_dict[tuple((a[i],b[i]))] += 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# state_codes = set(data['STATE'])\n",
    "# causes = list(set(data['STAT_CAUSE_DESCR']))\n",
    "# for code in state_codes:\n",
    "#     for cause in causes:\n",
    "#         if tuple((code,cause)) not in sum_cause_state_dict:\n",
    "#             sum_cause_state_dict[tuple((code,cause))] = 0\n",
    "\n",
    "# # Create the 13 features \n",
    "# for cause in causes:\n",
    "#     data['state_' +  cause + '_rate'] = np.zeros(data.shape[0],dtype='float64')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Relation Matrix between state and reason\n",
    "# ratio_mat = np.ndarray((data.shape[0],len(causes)),dtype='float64')\n",
    "\n",
    "# state_ratio_dict = {}\n",
    "# for state in state_codes:\n",
    "#     ratios = np.zeros(len(causes), dtype='float64')\n",
    "#     for i in range(len(causes)):\n",
    "#         ratios[i] = sum_cause_state_dict[tuple((state,causes[i]))]\n",
    "#     ratios = ratios / data[data['STATE'] == state].shape[0]\n",
    "#     state_ratio_dict[state] = ratios"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Filling Relation matrix\n",
    "# for i in range(data.shape[0]):\n",
    "#     ratio_mat[i] = state_ratio_dict[data['STATE'][i]]\n",
    "    \n",
    "# for i in range(len(causes)):\n",
    "#     data['state_' +  causes[i] + '_rate'] = ratio_mat[:, i]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Same processus with MATRIX ratios\n",
    "# d = data['OWNER_DESCR']\n",
    "# sum_owner_dict = {}\n",
    "# for e in d:\n",
    "#     if e not in sum_owner_dict:\n",
    "#         sum_owner_dict[e] = 1\n",
    "#     else:\n",
    "#         sum_owner_dict[e] += 1\n",
    "# fires_per_owner = np.zeros(d.shape[0])\n",
    "# data['fires_owner'] = [sum_owner_dict[val] for val in d]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# a = data['OWNER_DESCR']\n",
    "# b = data['STAT_CAUSE_DESCR']\n",
    "# sum_cause_owner_dict = {}\n",
    "# for i in range(a.shape[0]):\n",
    "#     if tuple((a[i],b[i])) not in sum_cause_owner_dict:\n",
    "#         sum_cause_owner_dict[tuple((a[i],b[i]))] = 1\n",
    "#     else:\n",
    "#         sum_cause_owner_dict[tuple((a[i],b[i]))] += 1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# owner_codes = set(data['OWNER_DESCR'])\n",
    "# causes = list(set(data['STAT_CAUSE_DESCR']))\n",
    "# for code in owner_codes:\n",
    "#     for cause in causes:\n",
    "#         if tuple((code,cause)) not in sum_cause_owner_dict:\n",
    "#             sum_cause_owner_dict[tuple((code,cause))] = 0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# ratio_mat = np.ndarray((data.shape[0],len(causes)),dtype='float64')\n",
    "# owner_ratio_dict = {}\n",
    "# for owner in owner_codes:\n",
    "#     ratios = np.zeros(len(causes), dtype='float64')\n",
    "#     for i in range(len(causes)):\n",
    "#         ratios[i] = sum_cause_owner_dict[tuple((owner,causes[i]))]\n",
    "#     ratios = ratios / data[data['OWNER_DESCR'] == owner].shape[0]\n",
    "#     owner_ratio_dict[owner] = ratios\n",
    "\n",
    "# for i in range(data.shape[0]):\n",
    "#     ratio_mat[i] = owner_ratio_dict[data['OWNER_DESCR'][i]]\n",
    "# for i in range(len(causes)):\n",
    "#     data['owner_' +  causes[i] + '_rate'] = ratio_mat[:, i]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Same processus for both features : Owner Description & States VS Reasons\n",
    "# Overfitted Feature\n",
    "grouped = data[['OWNER_DESCR','STATE','STAT_CAUSE_DESCR']].groupby(by=['OWNER_DESCR','STATE','STAT_CAUSE_DESCR'])\n",
    "\n",
    "\n",
    "# reminder : OWNER_DESCR = Name of primary owner or entity responsible for managing the land\n",
    "\n",
    "owner_state_cause_dict = {}\n",
    "owners = set(data['OWNER_DESCR'])\n",
    "states = set(data['STATE'])\n",
    "causes = set(data['STAT_CAUSE_DESCR'])\n",
    "             \n",
    "for owner in owners:\n",
    "    for state in states:\n",
    "        for cause in causes:\n",
    "            owner_state_cause_dict[tuple((owner,state,cause))] = 0\n",
    "for x,y in grouped:\n",
    "    owner_state_cause_dict[tuple((x[0],x[1],x[2]))] = y.shape[0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# df.groupby - A groupby operation involves some combination of splitting the object,\n",
    "# applying a function, and combining the results\n",
    "\n",
    "grouped = data[['OWNER_DESCR','STATE']].groupby(by=['OWNER_DESCR','STATE'])\n",
    "owner_state_dict = {}\n",
    "owners = set(data['OWNER_DESCR'])\n",
    "states = set(data['STATE'])\n",
    "             \n",
    "# For not divide by 0\n",
    "for owner in owners:\n",
    "    for state in states:\n",
    "        owner_state_dict[tuple((owner, state))] = 1\n",
    "len(owner_state_dict)\n",
    "\n",
    "for x,y in grouped:\n",
    "    owner_state_dict[tuple((x[0], x[1]))] = y.shape[0]"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-d2c6964a76a1>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mgrouped\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'OWNER_DESCR'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m'STATE'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgroupby\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mby\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'OWNER_DESCR'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;34m'STATE'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mowner_state_dict\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mowners\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'OWNER_DESCR'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ]
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "ratio_mat = np.ndarray((data.shape[0],len(causes)),dtype='float64')\n",
    "owner_state_ratio_dict = {}\n",
    "causes = list(causes)\n",
    "for owner in owners:\n",
    "    for state in states:\n",
    "        ratios = np.zeros(len(causes), dtype='float64')\n",
    "        sumi = 0\n",
    "        for i in range(len(causes)):\n",
    "            ratios[i] = owner_state_cause_dict[tuple((owner,state,causes[i]))]\n",
    "            sumi += owner_state_cause_dict[tuple((owner,state,causes[i]))]\n",
    "        ratios = ratios / owner_state_dict[tuple((owner,state))]\n",
    "        owner_state_ratio_dict[tuple((owner,state))] = ratios\n",
    "        \n",
    "for i in range(data.shape[0]):\n",
    "    owner = data['OWNER_DESCR'][i]\n",
    "    state = data['STATE'][i]\n",
    "    ratio_mat[i] = owner_state_ratio_dict[tuple((owner,state))]\n",
    "    \n",
    "a = pd.DataFrame(data[['OWNER_DESCR','STATE','STAT_CAUSE_DESCR']])\n",
    "for i in range(len(causes)):\n",
    "    data['owner_state_' +  causes[i] + '_rate'] = ratio_mat[:, i]\n",
    "    a['owner_state_' +  causes[i] + '_rate'] = ratio_mat[:, i]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "# Geographic feature\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from shapely.geometry import Point # Point class\n",
    "from shapely.geometry import shape # shape() is a function to convert geo objects through the interface\n",
    "import gc\n",
    "\n",
    "a = data[['LATITUDE','LONGITUDE']]\n",
    "fires = gpd.GeoDataFrame(a.drop(['LATITUDE', 'LONGITUDE'], axis=1),\n",
    "                       crs={'init': 'epsg:4326'},\n",
    "                       geometry=[shapely.geometry.Point(xy)\n",
    "                                 for xy in zip(a.LONGITUDE, a.LATITUDE)])\n",
    "cities = gpd.read_file('/kaggle/input/cities/tl_2017_us_uac10.shp')\n",
    "\n",
    "fires_in_cities = gpd.sjoin(fires, cities, how=\"left\", op='within')\n",
    "a = fires_in_cities['FUNCSTAT10'].replace('S',1)\n",
    "a = a.replace(np.nan,0)\n",
    "del cities\n",
    "gc.collect()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "d = data['STATE']\n",
    "sum_state_dict = {} # number of fires in each state\n",
    "for e in d:\n",
    "    if e not in sum_state_dict:\n",
    "        sum_state_dict[e] = 1\n",
    "    else:\n",
    "        sum_state_dict[e] += 1\n",
    "fires_per_state = np.zeros(d.shape[0])\n",
    "data['fires_in_state'] = [sum_state_dict[val] for val in d] # new feature"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# FINAL DATA \nlabel = clean_data_v1['STAT_CAUSE_DESCR']\n\nfinal_df = clean_data_v1[\"cyclic_month_of_year\"]\nfinal_df = pd.concat([final_df, size_class_dummies], axis=1)\nfinal_df = pd.concat([final_df, data.iloc[:, 39:52]], axis=1)\nfinal_df['cyclic_day_of_week'] = clean_data_v1['cyclic_day_of_week']\nfinal_df['cyclic_day_of_year'] = clean_data_v1['cyclic_day_of_year'] \nfinal_df['hour_in_day'] = hours['CONT_TIME']\nfinal_df['fire_period'] = fire_period\nfinal_df['is_urban'] = a\nfinal_df['is_holidays'] = df_holidays['is_holiday'].astype(int)\nfinal_df['num_fires_in_state'] = data['fires_in_state']\nfinal_df",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**MODELS**"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split\n\n#Import scikit-learn metrics module for accuracy calculation \nfrom sklearn import metrics \nfrom sklearn.metrics import zero_one_loss \nfrom sklearn.model_selection import GridSearchCV \n\nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.linear_model import LogisticRegressionCV \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import average_precision_score\n\nnp.random.seed(137)\n\nlabel=data['STAT_CAUSE_DESCR']\nX_train, X_test, y_train, y_test = train_test_split(final_df, label, test_size=0.25) # 75% training and 25% test\n\n#Create a Gaussian Classifier \nclf = xgb.XGBClassifier(max_depth=22,n_estimators=150)\n# clf = RandomForestClassifier(max_depth=20, n_estimators = 150)\nclf.fit(X_train,y_train)\n\n# X_train.info()\n#Train the model using the training sets \ny_pred=clf.predict(X_test)\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n# # average_precision = dict()\n# n_classes = 7\n# for i in range(n_classes):\n#     average_precision[i] = average_precision_score(y_test[:, i], y_pred[:, i])\n\n# A \"micro-average\": quantifying score on all classes jointly\n# average_precision[\"micro\"] = average_precision_score(y_test, y_pred,\n#                                                      average=\"micro\")\n# print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n#       .format(average_precision[\"micro\"]))\n# y_pred\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "from sklearn.metrics import precision_score\nprecision_score(y_test, y_pred, average='macro')",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# import catboost as cb\n# import numpy as np\n# from sklearn.datasets import load_breast_cancer\n# from sklearn.metrics import accuracy_score\n# from sklearn.model_selection import train_test_split\n\n# import optuna\n\n\n# def objective(trial):\n#     label=data['STAT_CAUSE_CODE']\n#     train_x, valid_x, train_y, valid_y = train_test_split(final_df, label, test_size=0.3)\n\n#     param = {\n#         \"objective\": trial.suggest_categorical(\"objective\", [\"MultiClass\"]),\n#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n#         \"depth\": trial.suggest_int(\"depth\", 1, 12),\n#         \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n#         \"bootstrap_type\": trial.suggest_categorical(\n#             \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n#         )\n#     }\n\n#     if param[\"bootstrap_type\"] == \"Bayesian\":\n#         param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n#     elif param[\"bootstrap_type\"] == \"Bernoulli\":\n#         param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n#     gbm = cb.CatBoostClassifier(**param)\n#     print('before fit')\n#     gbm.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=0, early_stopping_rounds=100)\n#     print('after fit')\n\n#     preds = gbm.predict(valid_x)\n#     pred_labels = np.rint(preds)\n#     accuracy = accuracy_score(valid_y, pred_labels)\n#     print(accuracy)\n#     print(\"Precision:\", precision_score(valid_y, pred_labels, average='macro'))\n#     return accuracy\n\n\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=10, show_progress_bar=True)\n\n# print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n# print(\"Best trial:\")\n# trial = study.best_trial\n\n# print(\"  Value: {}\".format(trial.value))\n\n# print(\"  Params: \")\n# for key, value in trial.params.items():\n#     print(\"    {}: {}\".format(key, value))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# # Gradient Boosting Model\n# from sklearn.model_selection import train_test_split\n# from sklearn import metrics \n# from sklearn.ensemble import GradientBoostingClassifier \n# from sklearn.model_selection import cross_val_score\n# import optuna \n# from optuna import trial\n# from sklearn.metrics import precision_score\n# from sklearn.pipeline import Pipeline\n\n# label=data['STAT_CAUSE_DESCR']\n# X_train, X_test, y_train, y_test = train_test_split(final_df, label, test_size=0.25)\n# # model = Pipeline([('gbm',GradientBoostingClassifier())])\n\n# def objective(trial):\n\n#     n_estimators =  trial.suggest_int('n_estimators', 2, 20)\n# #     max_features = trial.suggest_int('max_features', 1,20)\n# #     subsample = trial.suggest_uniform('subsample', 0.1, 0.5)\n    \n#     params = {\n#             'n_estimators': n_estimators\n# #             'max_features': max_features,\n# #             'subsample': subsample\n#             }\n#     model = GradientBoostingClassifier()\n#     model.set_params(**params)\n#     return - np.mean(cross_val_score(model, X_train, y_train, cv=8))\n\n# def Optimized_GradientBoostingClassifier():\n#     # optimize hyperparameters\n#     study = optuna.create_study(direction='maximize')\n#     study = study.optimize(objective, n_trials=2, timeout=600, show_progress_bar=True)\n\n#     model.set_params(**study.best_params)\n#     model.fit(X_train, y_train)\n#     y_pred=model.predict(X_test)\n#     print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n#     print(\"Precision:\", precision_score(y_test, y_pred, average='macro'))\n\n# # def XGB():\n# #     clf = xgboost.XGBClassifier().fit(X_train, y_train)\n# Optimized_GradientBoostingClassifier()",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# import catboost as cb\n# import numpy as np\n# from sklearn.datasets import load_breast_cancer\n# from sklearn.metrics import accuracy_score\n# from sklearn.model_selection import train_test_split\n\n# import optuna\n\n\n# def objective(trial):\n#     label=data['STAT_CAUSE_CODE']\n#     train_x, valid_x, train_y, valid_y = train_test_split(final_df, label, test_size=0.3)\n\n#     param = {\n#         \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n#         \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n#         \"depth\": trial.suggest_int(\"depth\", 1, 12),\n#         \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n#         \"bootstrap_type\": trial.suggest_categorical(\n#             \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n#         ),\n#         \"used_ram_limit\": \"3gb\",\n#         \"loss_function\" : 'MultiClass'\n#     }\n\n#     if param[\"bootstrap_type\"] == \"Bayesian\":\n#         param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n#     elif param[\"bootstrap_type\"] == \"Bernoulli\":\n#         param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n#     gbm = cb.CatBoostClassifier(**param)\n\n#     gbm.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=0, early_stopping_rounds=100)\n\n#     preds = gbm.predict(valid_x)\n#     pred_labels = np.rint(preds)\n#     accuracy = accuracy_score(valid_y, pred_labels)\n#     return accuracy\n\n\n# if __name__ == \"__main__\":\n#     study = optuna.create_study(direction=\"maximize\")\n#     study.optimize(objective, n_trials=10, timeout=600, show_progress_bar=True)\n\n#     print(\"Number of finished trials: {}\".format(len(study.trials)))\n\n#     print(\"Best trial:\")\n#     trial = study.best_trial\n\n#     print(\"  Value: {}\".format(trial.value))\n\n#     print(\"  Params: \")\n#     for key, value in trial.params.items():\n#         print(\"    {}: {}\".format(key, value))",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# import seaborn as sns\n# import matplotlib.pyplot as plt\n# from datetime import datetime\n# import statistics\n# clean_data_heat = clean_data_v1[['FIRE_YEAR', 'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME',\n#        'STAT_CAUSE_CODE', 'CONT_DATE', 'CONT_DOY',\n#        'CONT_TIME', 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE']]\n# heat_names =['FIRE_YEAR', 'DISCOVERY_DATE', 'DISCOVERY_DOY', 'DISCOVERY_TIME',\n#        'STAT_CAUSE_CODE', 'CONT_DATE', 'CONT_DOY',\n#        'CONT_TIME', 'FIRE_SIZE', 'LATITUDE', 'LONGITUDE']\n\n# ###\n# detection_hours_array = np.array(np.array((clean_data_heat['CONT_TIME']).dropna(),dtype ='int32')/100,dtype ='int32')\n# detection_hours_mode = statistics.mode(detection_hours_array)\n# ###\n# print(set(((clean_data_heat['CONT_TIME']).dropna())))\n# ###\n# contained_hours_array = np.array(np.array((clean_data_heat['DISCOVERY_TIME']).dropna(),dtype ='int32')/100,dtype ='int32')\n# contained_hours_mode = statistics.mode(contained_hours_array)\n# ###\n# cur = pd.DataFrame(detection_hours_array,columns=['hour'])\n# a = sns.countplot('hour',data=cur)\n# a.set_xticklabels(set(cur['hour']),rotation=45)\n# # \n\n# # for cont_date \n# #     df_test['Difference'] = df_test['First_Date'].sub(df_test['Second Date'], axis=0)\n\n# #     df_test['CONT_DAY'] = df_test['DISCOVERY_DATE'].sub(df_test['CONT_DATE'], axis=0)\n\n\n\n# clean_data_v1 = clean_data_v1.dropna()\n# f,ax=plt.subplots(1,1,figsize=(20,10))\n# sns.heatmap(np.abs(clean_data_v1.corr()),annot=True,cmap='RdYlGn',ax=ax)\n\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}